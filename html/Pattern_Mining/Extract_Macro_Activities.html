<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1, minimum-scale=1" name="viewport"/>
    <meta content="pdoc 0.9.2" name="generator"/>
    <title>Pattern_Mining.Extract_Macro_Activities API documentation</title>
    <meta content="" name="description"/>
    <link as="style" crossorigin href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css"
          integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" rel="preload stylesheet">
    <link as="style" crossorigin
          href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css"
          integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" rel="preload stylesheet">
    <link as="style" crossorigin href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css"
          rel="stylesheet preload">
    <style>
        :root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}
    </style>
    <style media="screen and (min-width: 700px)">
        @media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}
    </style>
    <style media="print">
        @media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}
    </style>
    <script crossorigin defer integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8="
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
    <script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
    <article id="content">
        <header>
            <h1 class="title">Module <code>Pattern_Mining.Extract_Macro_Activities</code></h1>
        </header>
        <section id="section-intro">
            <details class="source">
                <summary>
                    <span>Expand source code</span>
                </summary>
                <pre><code class="python">import errno
import math
import multiprocessing as mp
import time as t

import seaborn as sns
from scipy.signal import argrelextrema
from sklearn.mixture import GaussianMixture
from statsmodels.nonparametric.kde import KDEUnivariate

from Pattern_Mining import FP_growth, Candidate_Study
from Utils import *

sns.set_style(&#39;darkgrid&#39;)


def main():
    &#34;&#34;&#34;
    Check the distribution of frequency vs periodicity
    :return:
    &#34;&#34;&#34;

    dataset_name = &#39;bped_ramon&#39;
    output_folder = &#39;../output/{}/&#39;.format(dataset_name)
    dataset = pick_dataset(dataset_name, nb_days=-1)

    # TIME WINDOW PARAMETERS
    nb_days_per_window = 30
    time_window_duration = dt.timedelta(days=nb_days_per_window)
    start_date = dataset.date.min().to_pydatetime()
    end_date = dataset.date.max().to_pydatetime() - time_window_duration

    # SIM_MODEL PARAMETERS
    period = dt.timedelta(days=1)
    tep = 30
    support_min = 10
    # support_min = 3

    nb_processes = 2 * mp.cpu_count()  # For parallel computing

    if not os.path.exists(os.path.dirname(output_folder)):
        try:
            os.makedirs(os.path.dirname(output_folder))
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise

    start_time = t.time()
    print(&#39;##########################################&#39;)
    print(&#39;## MACRO ACTIVITIES EXTRACTION : {} ##&#39;.format(dataset_name.upper()))
    print(&#39;##########################################&#39;)

    macro_activities = extract_macro_activities(dataset=dataset, support_min=support_min, tep=tep,
                                                display=True, verbose=True)

    # results_df.to_csv(f&#39;../output/{dataset_name}/habits_results.csv&#39;, index=False)
    # monitoring_start_time = t.time()

    # nb_tw = math.floor((end_date - start_date) / period)  # Number of time windows available
    #
    # results = {}
    #
    # args = [(dataset, support_min, tep, period, time_window_duration, tw_id) for tw_id in range(nb_tw)]
    #
    # with mp.Pool(processes=nb_processes) as pool:
    #     mp_results = pool.starmap(extract_tw_macro_activities, args)
    #
    #     for result in mp_results:
    #         results[result[0]] = result[1]
    #
    # print(&#34;All Time Windows Treated&#34;)
    #
    # elapsed_time = dt.timedelta(seconds=round(t.time() - start_time, 1))
    #
    # print(&#34;###############################&#34;)
    # print(&#34;Elapsed time : {}&#34;.format(elapsed_time))
    #
    # pickle_out = open(output_folder + &#39;all_macro_activities&#39;, &#39;wb&#39;)
    # pickle.dump(results, pickle_out)
    # pickle_out.close()
    #
    # results = pickle.load(open(output_folder+&#39;all_macro_activities&#39;, &#39;rb&#39;))
    #
    # print(results)

    # plt.plot(np.arange(len(new_episodes_evol)), new_episodes_evol)
    # plt.title(&#39;Number of macro discovered&#39;)
    # plt.xlabel(&#39;Time Windows&#39;)
    # plt.ylabel(&#39;nb episodes&#39;)
    # plt.show()


def extract_tw_macro_activities(dataset, support_min, tep, period, window_duration, tw_id):
    &#34;&#34;&#34;
    Extract macro activities from a time window
    :param dataset:
    :param support_min:
    :param tep:
    :param period:
    :param window_duration:
    :param tw_id:
    :return: tw_id (id of the time window), macro_activities_episode (A dict-like object with &#39;episode&#39; as key and the
    tuple of dataframes (episode_occurrences, events) as value)
    &#34;&#34;&#34;

    print(&#39;Time window {} Macro-Activities screening started...&#39;.format(tw_id))
    start_date = dataset.date.min().to_pydatetime()

    window_start_date = start_date + tw_id * period
    window_end_date = window_start_date + window_duration

    tw_dataset = dataset.loc[(dataset.date &gt;= window_start_date) &amp; (dataset.date &lt; window_end_date)].copy()

    tw_macro_activities_episodes = extract_macro_activities(dataset=tw_dataset, support_min=support_min, tep=tep,
                                                            display=False, verbose=False)

    print(&#39;Time window {} screening finished.&#39;.format(tw_id))

    return tw_id, tw_macro_activities_episodes

    # results[tw_id] = tw_macro_activities
    #
    #


def extract_macro_activities(dataset, support_min, tep, verbose=False, display=False, singles_only=False):
    &#34;&#34;&#34;
    :param display:
    :param dataset:
    :param support_min:
    :param tep:
    :param period:
    :param verbose:
    :param singles_only:
    :return:
    &#34;&#34;&#34;

    macro_activities = {}

    results_df = pd.DataFrame(columns=[&#39;episode&#39;, &#39;description&#39;, &#39;nb_occ&#39;, &#39;accuracy&#39;, &#39;start_validity&#39;,
                                       &#39;end_validity&#39;, &#39;nb_days_val&#39;])

    i = 0
    while len(dataset) &gt; 0 and not singles_only:
        i += 1
        # episode, nb_occ, ratio, score = find_best_episode(log_dataset=log_dataset, tep=tep, support_min=support_min,
        #                                                   period=period, display=display)
        # Most frequents episode

        best_episode, nb_occ, _, accuracy, description, start_val, end_val = find_best_episode(dataset=dataset, tep=tep,
                                                                                               support_min=support_min,
                                                                                               display=display,
                                                                                               verbose=verbose)

        if best_episode is None:
            print(&#34;**********No more best episode found*************&#34;)
            break

        episode_occurrences, events = compute_episode_occurrences(dataset=dataset, episode=best_episode, tep=tep)

        macro_activities[tuple(best_episode)] = (episode_occurrences, events)

        nb_days_val = int((end_val - start_val) / dt.timedelta(days=1))
        results_df.loc[len(results_df)] = [list(best_episode), description, nb_occ, accuracy, start_val, end_val,
                                           nb_days_val]
        if verbose:
            print(&#34;########################################&#34;)
            print(&#34;Run N°{}&#34;.format(i))
            print(&#34;Best episode found {}.&#34;.format(best_episode))
            print(f&#34;Nb Occurrences : \t{nb_occ}&#34;)
            print(f&#34;Accuracy : \t{accuracy:.2f}&#34;)
            print(f&#34;Description: \t{description}&#34;)
            print(f&#34;Validity Period : {str(start_val)} -- {str(end_val)}&#34;)
            print(f&#34;Number Days of validity : {nb_days_val}&#34;)


        dataset = pd.concat([dataset, events]).drop_duplicates(keep=False)

    # Mining of the rest of the dataset by creating single-activities
    labels = dataset.label.unique()

    for label in labels:
        events = dataset[dataset.label == label].copy()
        episode_occurrences = events[[&#39;date&#39;, &#39;end_date&#39;]].copy()

        macro_activities[(label,)] = (episode_occurrences, events)

        if verbose:
            print(&#34;########################################&#34;)
            # print(&#34;Run N°{}&#34;.format(i))
            print(&#34;Best episode found {}.&#34;.format((label,)))
            print(&#34;Nb Occurrences : \t{}&#34;.format(len(events)))


    return macro_activities


def find_best_episode(dataset, tep, support_min, display=False, verbose=False):
    &#34;&#34;&#34;
    Find the best episode in a event log
    :param verbose:
    :param dataset: Event log
    :param tep: duration max of an episode
    :param support_min: number of occurrences min in the event log
    :param period: periodicity of the analysis
    :param display : display the solutions
    :return: the best episode found
    &#34;&#34;&#34;

    # Dataset to store the objective values of the solutions
    comparaison_df = pd.DataFrame(
        columns=[&#39;episode&#39;, &#39;nb_occ&#39;, &#39;nb_events&#39;, &#39;ratio_dataset&#39;, &#39;accuracy&#39;, &#39;power&#39;, &#39;start_val&#39;, &#39;end_val&#39;])

    # Most frequents episode
    frequent_episodes = FP_growth.find_frequent_episodes(dataset, support_min, tep)

    if len(frequent_episodes) == 0:
        print(&#34;No frequent episodes found&#34;)
        return None, None, None, None, None, None, None

    GMM_descriptions = {}
    # Compute the sparsity of the episode occurrence time
    for episode, _ in frequent_episodes.items():
        episode = sorted(episode, reverse=True)

        periodicity = Candidate_Study.periodicity_search(data=dataset, episode=episode, delta_Tmax_ratio=3,
                                                         support_min=support_min, std_max=0.1, tolerance_ratio=2,
                                                         Tep=tep, display=False, verbose=False)

        if periodicity is not None:
            nb_occ = periodicity[&#39;nb_occ&#39;]
            ratio_dataset = len(episode) * nb_occ / len(dataset)  # ratio in the log_dataset
            comparaison_df.loc[len(comparaison_df)] = [list(episode), nb_occ, len(episode) * nb_occ, ratio_dataset,
                                                       periodicity[&#39;accuracy&#39;], periodicity[&#39;compression_power&#39;],
                                                       periodicity[&#39;delta_t&#39;][0], periodicity[&#39;delta_t&#39;][1]]
            GMM_descriptions[tuple(episode)] = periodicity[&#39;description&#39;]

    if len(comparaison_df) == 0:
        print(&#34;*****No periodicities Found******&#34;)
        return None, None, None, None, None, None, None

    scores = comparaison_df[[&#34;nb_events&#34;, &#34;accuracy&#34;]].values
    scores = np.asarray(scores)

    # Compute the pareto front
    pareto = identify_pareto(scores)
    pareto_front_df = comparaison_df.loc[pareto]

    pareto_front_df.sort_values([&#39;ratio_dataset&#39;, &#39;accuracy&#39;], ascending=False, inplace=True)

    ##############################################
    #       MOST INTERESTING EPISODE ??          #
    ##############################################

    max_distance = 0
    best_point = None

    for i, row in pareto_front_df.iterrows():
        # ratio_dataset = row[&#39;ratio_dataset&#39;]
        # score = row[&#39;score&#39;]
        dist = row[&#39;power&#39;]
        # dist = math.pow(point[0], point[1])

        if dist &gt; max_distance:
            max_distance = dist
            best_point = row

    if display:
        sns.scatterplot(x=&#39;nb_events&#39;, y=&#39;accuracy&#39;, size=&#39;nb_occ&#39;, data=comparaison_df)
        plt.plot(pareto_front_df.nb_events, pareto_front_df.accuracy, color=&#39;r&#39;, label=&#39;Pareto Front&#39;)
        plt.plot([best_point[&#39;nb_events&#39;]], [best_point[&#39;accuracy&#39;]], marker=&#39;x&#39;, color=&#34;red&#34;)

        #
        for _, row in pareto_front_df.iterrows():
            plt.text(row[&#39;nb_events&#39;], row[&#39;accuracy&#39;], row[&#39;episode&#39;], verticalalignment=&#39;top&#39;,
                     size=&#39;large&#39;, color=&#39;black&#39;, weight=&#39;semibold&#39;)

        plt.legend(loc=2)
        # plt.ylim((0,1))
        plt.xlabel(&#39;Nombre d\&#39;évènements&#39;, fontsize=18)
        plt.ylabel(&#39;Précision de la périodicité&#39;, fontsize=18)
        plt.title(&#39;Selection du meilleur épisode&#39;, fontsize=20)
        plt.show()

    return best_point[&#39;episode&#39;], best_point[&#39;nb_occ&#39;], best_point[&#39;ratio_dataset&#39;], best_point[&#39;accuracy&#39;], \
           GMM_descriptions[tuple(best_point[&#39;episode&#39;])], best_point[&#39;start_val&#39;], best_point[&#39;end_val&#39;]


def compute_episode_description(dataset, episode, period, tep):
    &#34;&#34;&#34;
    Compute the frequency description of the episode
    :param dataset:
    :param episode:
    :param tep:
    :return: a dict-like object {[mu] : sigma}
    &#34;&#34;&#34;

    # find the episode occurrences
    occurrences = Candidate_Study.find_occurrences(dataset, episode, tep)

    # Relative timestamp in the period
    occurrences[&#34;relative_date&#34;] = occurrences.date.apply(
        lambda x: Candidate_Study.modulo_datetime(x.to_pydatetime(), period))

    data_points = occurrences.relative_date.values

    # # For midnight-morning issue
    # data_points_2 = [x + period.total_seconds() for x in data_points]
    #
    # big_data_points = np.asarray(list(data_points) + list(data_points_2)).reshape(-1, 1)

    # Find the number of clusters
    kde_a = KDEUnivariate(data_points)
    kde_a.fit(bw=&#34;normal_reference&#34;)

    day_bins = np.linspace(0, period.total_seconds(), 1000)
    density_values = kde_a.evaluate(day_bins)

    mi, ma = argrelextrema(density_values, np.less)[0], argrelextrema(density_values, np.greater)[0]

    nb_clusters = len(day_bins[ma])
    #
    # Fit Gaussian Mixture Model
    GMM = GaussianMixture(n_components=nb_clusters, n_init=10).fit(data_points.reshape(-1, 1))

    # Compute the description
    GMM_descr = {}
    for i in range(len(GMM.means_)):
        mu = int(GMM.means_[i][0]) % period.total_seconds()
        sigma = int(math.ceil(np.sqrt(GMM.covariances_[i])))

        GMM_descr[mu] = sigma

    return GMM_descr


def compute_episode_occurrences(dataset, episode, tep):
    &#34;&#34;&#34;
    Compute the episode occurrences in the log_dataset
    :param dataset:
    :param episode:
    :param tep:
    :return:
    &#34;&#34;&#34;

    data = dataset[dataset.label.isin(episode)].copy()

    if len(episode) == 1:
        return data, data

    events = pd.DataFrame(columns=[&#34;date&#34;, &#34;end_date&#34;, &#34;label&#34;])
    episode_occurrences = Candidate_Study.find_occurrences(data, episode, tep)

    for index, occurrence in episode_occurrences.iterrows():
        start_date = occurrence[&#34;date&#34;]
        end_date = start_date + dt.timedelta(minutes=tep)
        mini_data = data.loc[(data.date &gt;= start_date) &amp; (data.date &lt; end_date)].copy()

        mini_data.drop_duplicates([&#34;label&#34;], keep=&#39;first&#39;, inplace=True)
        events = events.append(mini_data, ignore_index=True)

    events.sort_values([&#34;date&#34;], ascending=True, inplace=True)

    return episode_occurrences, events


def identify_pareto(scores):
    &#34;&#34;&#34;
    identify the pareto front from data
    :param scores: 2D numpy array representing solutions
    :return: the list of period_ts_index for non-dominated solutions
    &#34;&#34;&#34;
    # Count number of items
    population_size = scores.shape[0]
    # Create a NumPy period_ts_index for scores on the pareto front (zero indexed)
    population_ids = np.arange(population_size)
    # Create a starting list of items on the Pareto front
    # All items start off as being labelled as on the Pareto front
    pareto_front = np.ones(population_size, dtype=bool)
    # Loop through each item. This will then be compared with all other items
    for i in range(population_size):
        # Loop through all other items
        for j in range(population_size):
            # Check if our &#39;i&#39; point is dominated by out &#39;j&#39; point
            if all(scores[j] &gt;= scores[i]) and any(scores[j] &gt; scores[i]):
                # j dominates i. Label &#39;i&#39; point as not on Pareto front
                pareto_front[i] = 0
                # Stop further comparisons with &#39;i&#39; (no more comparisons needed)
                break
    # Return ids of scenarios on pareto front
    return population_ids[pareto_front]


if __name__ == &#34;__main__&#34;:
    main()</code></pre>
            </details>
        </section>
        <section>
        </section>
        <section>
        </section>
        <section>
            <h2 class="section-title" id="header-functions">Functions</h2>
            <dl>
                <dt id="Pattern_Mining.Extract_Macro_Activities.compute_episode_description"><code class="name flex">
                    <span>def <span class="ident">compute_episode_description</span></span>(<span>dataset, episode, period, tep)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>Compute the frequency description of the episode
                        :param dataset:
                        :param episode:
                        :param tep:
                        :return: a dict-like object {[mu] : sigma}</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def compute_episode_description(dataset, episode, period, tep):
    &#34;&#34;&#34;
    Compute the frequency description of the episode
    :param dataset:
    :param episode:
    :param tep:
    :return: a dict-like object {[mu] : sigma}
    &#34;&#34;&#34;

    # find the episode occurrences
    occurrences = Candidate_Study.find_occurrences(dataset, episode, tep)

    # Relative timestamp in the period
    occurrences[&#34;relative_date&#34;] = occurrences.date.apply(
        lambda x: Candidate_Study.modulo_datetime(x.to_pydatetime(), period))

    data_points = occurrences.relative_date.values

    # # For midnight-morning issue
    # data_points_2 = [x + period.total_seconds() for x in data_points]
    #
    # big_data_points = np.asarray(list(data_points) + list(data_points_2)).reshape(-1, 1)

    # Find the number of clusters
    kde_a = KDEUnivariate(data_points)
    kde_a.fit(bw=&#34;normal_reference&#34;)

    day_bins = np.linspace(0, period.total_seconds(), 1000)
    density_values = kde_a.evaluate(day_bins)

    mi, ma = argrelextrema(density_values, np.less)[0], argrelextrema(density_values, np.greater)[0]

    nb_clusters = len(day_bins[ma])
    #
    # Fit Gaussian Mixture Model
    GMM = GaussianMixture(n_components=nb_clusters, n_init=10).fit(data_points.reshape(-1, 1))

    # Compute the description
    GMM_descr = {}
    for i in range(len(GMM.means_)):
        mu = int(GMM.means_[i][0]) % period.total_seconds()
        sigma = int(math.ceil(np.sqrt(GMM.covariances_[i])))

        GMM_descr[mu] = sigma

    return GMM_descr</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Extract_Macro_Activities.compute_episode_occurrences"><code class="name flex">
                    <span>def <span
                            class="ident">compute_episode_occurrences</span></span>(<span>dataset, episode, tep)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>Compute the episode occurrences in the log_dataset
                        :param dataset:
                        :param episode:
                        :param tep:
                        :return:</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def compute_episode_occurrences(dataset, episode, tep):
    &#34;&#34;&#34;
    Compute the episode occurrences in the log_dataset
    :param dataset:
    :param episode:
    :param tep:
    :return:
    &#34;&#34;&#34;

    data = dataset[dataset.label.isin(episode)].copy()

    if len(episode) == 1:
        return data, data

    events = pd.DataFrame(columns=[&#34;date&#34;, &#34;end_date&#34;, &#34;label&#34;])
    episode_occurrences = Candidate_Study.find_occurrences(data, episode, tep)

    for index, occurrence in episode_occurrences.iterrows():
        start_date = occurrence[&#34;date&#34;]
        end_date = start_date + dt.timedelta(minutes=tep)
        mini_data = data.loc[(data.date &gt;= start_date) &amp; (data.date &lt; end_date)].copy()

        mini_data.drop_duplicates([&#34;label&#34;], keep=&#39;first&#39;, inplace=True)
        events = events.append(mini_data, ignore_index=True)

    events.sort_values([&#34;date&#34;], ascending=True, inplace=True)

    return episode_occurrences, events</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Extract_Macro_Activities.extract_macro_activities"><code class="name flex">
                    <span>def <span class="ident">extract_macro_activities</span></span>(<span>dataset, support_min, tep, verbose=False, display=False, singles_only=False)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>:param display:
                        :param dataset:
                        :param support_min:
                        :param tep:
                        :param period:
                        :param verbose:
                        :param singles_only:
                        :return:</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def extract_macro_activities(dataset, support_min, tep, verbose=False, display=False, singles_only=False):
    &#34;&#34;&#34;
    :param display:
    :param dataset:
    :param support_min:
    :param tep:
    :param period:
    :param verbose:
    :param singles_only:
    :return:
    &#34;&#34;&#34;

    macro_activities = {}

    results_df = pd.DataFrame(columns=[&#39;episode&#39;, &#39;description&#39;, &#39;nb_occ&#39;, &#39;accuracy&#39;, &#39;start_validity&#39;,
                                       &#39;end_validity&#39;, &#39;nb_days_val&#39;])

    i = 0
    while len(dataset) &gt; 0 and not singles_only:
        i += 1
        # episode, nb_occ, ratio, score = find_best_episode(log_dataset=log_dataset, tep=tep, support_min=support_min,
        #                                                   period=period, display=display)
        # Most frequents episode

        best_episode, nb_occ, _, accuracy, description, start_val, end_val = find_best_episode(dataset=dataset, tep=tep,
                                                                                               support_min=support_min,
                                                                                               display=display,
                                                                                               verbose=verbose)

        if best_episode is None:
            print(&#34;**********No more best episode found*************&#34;)
            break

        episode_occurrences, events = compute_episode_occurrences(dataset=dataset, episode=best_episode, tep=tep)

        macro_activities[tuple(best_episode)] = (episode_occurrences, events)

        nb_days_val = int((end_val - start_val) / dt.timedelta(days=1))
        results_df.loc[len(results_df)] = [list(best_episode), description, nb_occ, accuracy, start_val, end_val,
                                           nb_days_val]
        if verbose:
            print(&#34;########################################&#34;)
            print(&#34;Run N°{}&#34;.format(i))
            print(&#34;Best episode found {}.&#34;.format(best_episode))
            print(f&#34;Nb Occurrences : \t{nb_occ}&#34;)
            print(f&#34;Accuracy : \t{accuracy:.2f}&#34;)
            print(f&#34;Description: \t{description}&#34;)
            print(f&#34;Validity Period : {str(start_val)} -- {str(end_val)}&#34;)
            print(f&#34;Number Days of validity : {nb_days_val}&#34;)


        dataset = pd.concat([dataset, events]).drop_duplicates(keep=False)

    # Mining of the rest of the dataset by creating single-activities
    labels = dataset.label.unique()

    for label in labels:
        events = dataset[dataset.label == label].copy()
        episode_occurrences = events[[&#39;date&#39;, &#39;end_date&#39;]].copy()

        macro_activities[(label,)] = (episode_occurrences, events)

        if verbose:
            print(&#34;########################################&#34;)
            # print(&#34;Run N°{}&#34;.format(i))
            print(&#34;Best episode found {}.&#34;.format((label,)))
            print(&#34;Nb Occurrences : \t{}&#34;.format(len(events)))


    return macro_activities</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Extract_Macro_Activities.extract_tw_macro_activities"><code class="name flex">
                    <span>def <span class="ident">extract_tw_macro_activities</span></span>(<span>dataset, support_min, tep, period, window_duration, tw_id)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>Extract macro activities from a time window
                        :param dataset:
                        :param support_min:
                        :param tep:
                        :param period:
                        :param window_duration:
                        :param tw_id:
                        :return: tw_id (id of the time window), macro_activities_episode (A dict-like object with
                        'episode' as key and the
                        tuple of dataframes (episode_occurrences, events) as value)</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def extract_tw_macro_activities(dataset, support_min, tep, period, window_duration, tw_id):
    &#34;&#34;&#34;
    Extract macro activities from a time window
    :param dataset:
    :param support_min:
    :param tep:
    :param period:
    :param window_duration:
    :param tw_id:
    :return: tw_id (id of the time window), macro_activities_episode (A dict-like object with &#39;episode&#39; as key and the
    tuple of dataframes (episode_occurrences, events) as value)
    &#34;&#34;&#34;

    print(&#39;Time window {} Macro-Activities screening started...&#39;.format(tw_id))
    start_date = dataset.date.min().to_pydatetime()

    window_start_date = start_date + tw_id * period
    window_end_date = window_start_date + window_duration

    tw_dataset = dataset.loc[(dataset.date &gt;= window_start_date) &amp; (dataset.date &lt; window_end_date)].copy()

    tw_macro_activities_episodes = extract_macro_activities(dataset=tw_dataset, support_min=support_min, tep=tep,
                                                            display=False, verbose=False)

    print(&#39;Time window {} screening finished.&#39;.format(tw_id))

    return tw_id, tw_macro_activities_episodes</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Extract_Macro_Activities.find_best_episode"><code class="name flex">
                    <span>def <span class="ident">find_best_episode</span></span>(<span>dataset, tep, support_min, display=False, verbose=False)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>Find the best episode in a event log
                        :param verbose:
                        :param dataset: Event log
                        :param tep: duration max of an episode
                        :param support_min: number of occurrences min in the event log
                        :param period: periodicity of the analysis
                        :param display : display the solutions
                        :return: the best episode found</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def find_best_episode(dataset, tep, support_min, display=False, verbose=False):
    &#34;&#34;&#34;
    Find the best episode in a event log
    :param verbose:
    :param dataset: Event log
    :param tep: duration max of an episode
    :param support_min: number of occurrences min in the event log
    :param period: periodicity of the analysis
    :param display : display the solutions
    :return: the best episode found
    &#34;&#34;&#34;

    # Dataset to store the objective values of the solutions
    comparaison_df = pd.DataFrame(
        columns=[&#39;episode&#39;, &#39;nb_occ&#39;, &#39;nb_events&#39;, &#39;ratio_dataset&#39;, &#39;accuracy&#39;, &#39;power&#39;, &#39;start_val&#39;, &#39;end_val&#39;])

    # Most frequents episode
    frequent_episodes = FP_growth.find_frequent_episodes(dataset, support_min, tep)

    if len(frequent_episodes) == 0:
        print(&#34;No frequent episodes found&#34;)
        return None, None, None, None, None, None, None

    GMM_descriptions = {}
    # Compute the sparsity of the episode occurrence time
    for episode, _ in frequent_episodes.items():
        episode = sorted(episode, reverse=True)

        periodicity = Candidate_Study.periodicity_search(data=dataset, episode=episode, delta_Tmax_ratio=3,
                                                         support_min=support_min, std_max=0.1, tolerance_ratio=2,
                                                         Tep=tep, display=False, verbose=False)

        if periodicity is not None:
            nb_occ = periodicity[&#39;nb_occ&#39;]
            ratio_dataset = len(episode) * nb_occ / len(dataset)  # ratio in the log_dataset
            comparaison_df.loc[len(comparaison_df)] = [list(episode), nb_occ, len(episode) * nb_occ, ratio_dataset,
                                                       periodicity[&#39;accuracy&#39;], periodicity[&#39;compression_power&#39;],
                                                       periodicity[&#39;delta_t&#39;][0], periodicity[&#39;delta_t&#39;][1]]
            GMM_descriptions[tuple(episode)] = periodicity[&#39;description&#39;]

    if len(comparaison_df) == 0:
        print(&#34;*****No periodicities Found******&#34;)
        return None, None, None, None, None, None, None

    scores = comparaison_df[[&#34;nb_events&#34;, &#34;accuracy&#34;]].values
    scores = np.asarray(scores)

    # Compute the pareto front
    pareto = identify_pareto(scores)
    pareto_front_df = comparaison_df.loc[pareto]

    pareto_front_df.sort_values([&#39;ratio_dataset&#39;, &#39;accuracy&#39;], ascending=False, inplace=True)

    ##############################################
    #       MOST INTERESTING EPISODE ??          #
    ##############################################

    max_distance = 0
    best_point = None

    for i, row in pareto_front_df.iterrows():
        # ratio_dataset = row[&#39;ratio_dataset&#39;]
        # score = row[&#39;score&#39;]
        dist = row[&#39;power&#39;]
        # dist = math.pow(point[0], point[1])

        if dist &gt; max_distance:
            max_distance = dist
            best_point = row

    if display:
        sns.scatterplot(x=&#39;nb_events&#39;, y=&#39;accuracy&#39;, size=&#39;nb_occ&#39;, data=comparaison_df)
        plt.plot(pareto_front_df.nb_events, pareto_front_df.accuracy, color=&#39;r&#39;, label=&#39;Pareto Front&#39;)
        plt.plot([best_point[&#39;nb_events&#39;]], [best_point[&#39;accuracy&#39;]], marker=&#39;x&#39;, color=&#34;red&#34;)

        #
        for _, row in pareto_front_df.iterrows():
            plt.text(row[&#39;nb_events&#39;], row[&#39;accuracy&#39;], row[&#39;episode&#39;], verticalalignment=&#39;top&#39;,
                     size=&#39;large&#39;, color=&#39;black&#39;, weight=&#39;semibold&#39;)

        plt.legend(loc=2)
        # plt.ylim((0,1))
        plt.xlabel(&#39;Nombre d\&#39;évènements&#39;, fontsize=18)
        plt.ylabel(&#39;Précision de la périodicité&#39;, fontsize=18)
        plt.title(&#39;Selection du meilleur épisode&#39;, fontsize=20)
        plt.show()

    return best_point[&#39;episode&#39;], best_point[&#39;nb_occ&#39;], best_point[&#39;ratio_dataset&#39;], best_point[&#39;accuracy&#39;], \
           GMM_descriptions[tuple(best_point[&#39;episode&#39;])], best_point[&#39;start_val&#39;], best_point[&#39;end_val&#39;]</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Extract_Macro_Activities.identify_pareto"><code class="name flex">
                    <span>def <span class="ident">identify_pareto</span></span>(<span>scores)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>identify the pareto front from data
                        :param scores: 2D numpy array representing solutions
                        :return: the list of period_ts_index for non-dominated solutions</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def identify_pareto(scores):
    &#34;&#34;&#34;
    identify the pareto front from data
    :param scores: 2D numpy array representing solutions
    :return: the list of period_ts_index for non-dominated solutions
    &#34;&#34;&#34;
    # Count number of items
    population_size = scores.shape[0]
    # Create a NumPy period_ts_index for scores on the pareto front (zero indexed)
    population_ids = np.arange(population_size)
    # Create a starting list of items on the Pareto front
    # All items start off as being labelled as on the Pareto front
    pareto_front = np.ones(population_size, dtype=bool)
    # Loop through each item. This will then be compared with all other items
    for i in range(population_size):
        # Loop through all other items
        for j in range(population_size):
            # Check if our &#39;i&#39; point is dominated by out &#39;j&#39; point
            if all(scores[j] &gt;= scores[i]) and any(scores[j] &gt; scores[i]):
                # j dominates i. Label &#39;i&#39; point as not on Pareto front
                pareto_front[i] = 0
                # Stop further comparisons with &#39;i&#39; (no more comparisons needed)
                break
    # Return ids of scenarios on pareto front
    return population_ids[pareto_front]</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Extract_Macro_Activities.main"><code class="name flex">
                    <span>def <span class="ident">main</span></span>(<span>)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>Check the distribution of frequency vs periodicity
                        :return:</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def main():
    &#34;&#34;&#34;
    Check the distribution of frequency vs periodicity
    :return:
    &#34;&#34;&#34;

    dataset_name = &#39;bped_ramon&#39;
    output_folder = &#39;../output/{}/&#39;.format(dataset_name)
    dataset = pick_dataset(dataset_name, nb_days=-1)

    # TIME WINDOW PARAMETERS
    nb_days_per_window = 30
    time_window_duration = dt.timedelta(days=nb_days_per_window)
    start_date = dataset.date.min().to_pydatetime()
    end_date = dataset.date.max().to_pydatetime() - time_window_duration

    # SIM_MODEL PARAMETERS
    period = dt.timedelta(days=1)
    tep = 30
    support_min = 10
    # support_min = 3

    nb_processes = 2 * mp.cpu_count()  # For parallel computing

    if not os.path.exists(os.path.dirname(output_folder)):
        try:
            os.makedirs(os.path.dirname(output_folder))
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise

    start_time = t.time()
    print(&#39;##########################################&#39;)
    print(&#39;## MACRO ACTIVITIES EXTRACTION : {} ##&#39;.format(dataset_name.upper()))
    print(&#39;##########################################&#39;)

    macro_activities = extract_macro_activities(dataset=dataset, support_min=support_min, tep=tep,
                                                display=True, verbose=True)</code></pre>
                    </details>
                </dd>
            </dl>
        </section>
        <section>
        </section>
    </article>
    <nav id="sidebar">
        <h1>Index</h1>
        <div class="toc">
            <ul></ul>
        </div>
        <ul id="index">
            <li><h3>Super-module</h3>
                <ul>
                    <li><code><a href="index.html" title="Pattern_Mining">Pattern_Mining</a></code></li>
                </ul>
            </li>
            <li><h3><a href="#header-functions">Functions</a></h3>
                <ul class="">
                    <li><code><a href="#Pattern_Mining.Extract_Macro_Activities.compute_episode_description"
                                 title="Pattern_Mining.Extract_Macro_Activities.compute_episode_description">compute_episode_description</a></code>
                    </li>
                    <li><code><a href="#Pattern_Mining.Extract_Macro_Activities.compute_episode_occurrences"
                                 title="Pattern_Mining.Extract_Macro_Activities.compute_episode_occurrences">compute_episode_occurrences</a></code>
                    </li>
                    <li><code><a href="#Pattern_Mining.Extract_Macro_Activities.extract_macro_activities"
                                 title="Pattern_Mining.Extract_Macro_Activities.extract_macro_activities">extract_macro_activities</a></code>
                    </li>
                    <li><code><a href="#Pattern_Mining.Extract_Macro_Activities.extract_tw_macro_activities"
                                 title="Pattern_Mining.Extract_Macro_Activities.extract_tw_macro_activities">extract_tw_macro_activities</a></code>
                    </li>
                    <li><code><a href="#Pattern_Mining.Extract_Macro_Activities.find_best_episode"
                                 title="Pattern_Mining.Extract_Macro_Activities.find_best_episode">find_best_episode</a></code>
                    </li>
                    <li><code><a href="#Pattern_Mining.Extract_Macro_Activities.identify_pareto"
                                 title="Pattern_Mining.Extract_Macro_Activities.identify_pareto">identify_pareto</a></code>
                    </li>
                    <li><code><a href="#Pattern_Mining.Extract_Macro_Activities.main"
                                 title="Pattern_Mining.Extract_Macro_Activities.main">main</a></code></li>
                </ul>
            </li>
        </ul>
    </nav>
</main>
<footer id="footer">
    <p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>