<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1, minimum-scale=1" name="viewport"/>
    <meta content="pdoc 0.9.2" name="generator"/>
    <title>Pattern_Mining.Pattern_Discovery API documentation</title>
    <meta content="Created on Wed Mar 14 10:22:14 2018 â€¦" name="description"/>
    <link as="style" crossorigin href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css"
          integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" rel="preload stylesheet">
    <link as="style" crossorigin
          href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css"
          integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" rel="preload stylesheet">
    <link as="style" crossorigin href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css"
          rel="stylesheet preload">
    <style>
        :root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}
    </style>
    <style media="screen and (min-width: 700px)">
        @media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}
    </style>
    <style media="print">
        @media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}
    </style>
    <script crossorigin defer integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8="
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
    <script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
    <article id="content">
        <header>
            <h1 class="title">Module <code>Pattern_Mining.Pattern_Discovery</code></h1>
        </header>
        <section id="section-intro">
            <p>Created on Wed Mar 14 10:22:14 2018</p>
            <p>@author: cyriac.azefack</p>
            <details class="source">
                <summary>
                    <span>Expand source code</span>
                </summary>
                <pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
Created on Wed Mar 14 10:22:14 2018

@author: cyriac.azefack
&#34;&#34;&#34;

import errno
import pickle
import sys
import time as t
from optparse import OptionParser

from Pattern_Mining import FP_growth, Candidate_Study
from Utils import *


def main():
    ######################
    # DATA PREPROCESSING #
    ######################

    &#34;&#34;&#34;
    The dataframe should have 1 period_ts_index (date as datetime) and 1 feature (label)
    &#34;&#34;&#34;

    parser = OptionParser(usage=&#39;Usage of the Pattern Discovery algorihtm: %prog &lt;options&gt;&#39;)
    parser.add_option(&#39;-n&#39;, &#39;--dataset_name&#39;, help=&#39;Name of the Input event log&#39;, dest=&#39;dataset_name&#39;, action=&#39;store&#39;,
                      type=&#39;string&#39;)
    parser.add_option(&#39;-r&#39;, &#39;--replication&#39;, help=&#39;Replication Identifier&#39;, dest=&#39;replication&#39;, action=&#39;store&#39;,
                      type=int, default=0)
    parser.add_option(&#39;-o&#39;, &#39;--output_dir&#39;, help=&#39;Output directory&#39;, dest=&#39;output_dir&#39;, action=&#39;store&#39;, type=&#39;string&#39;)
    parser.add_option(&#39;-w&#39;, &#39;--window_size&#39;, help=&#39;Number of the days used&#39;, dest=&#39;window_size&#39;, action=&#39;store&#39;,
                      type=int, default=-1)
    parser.add_option(&#39;-s&#39;, &#39;--support&#39;, help=&#39;Minimum number of occurrences of a pattern&#39;, dest=&#39;support_min&#39;,
                      action=&#39;store&#39;, type=int, default=3)
    parser.add_option(&#39;-a&#39;, &#39;--accuracy_min&#39;, help=&#39;Accuracy min of a pattern&#39;, dest=&#39;accuracy_min&#39;, action=&#39;store&#39;,
                      type=float, default=0.5)
    parser.add_option(&#39;-t&#39;, &#39;--tep&#39;, help=&#39;Duration max of an occurrence (in minutes)&#39;, dest=&#39;tep&#39;, action=&#39;store&#39;,
                      type=int, default=30)

    (options, args) = parser.parse_args()
    # Mandatory Options
    if options.dataset_name is None:
        print(&#34;The name of the Input event log is missing\n&#34;)
        parser.print_help()
        exit(-1)

    dataset_name = options.dataset_name
    replication_id = options.replication

    if options.output_dir is None:
        my_path = os.path.abspath(os.path.dirname(__file__))
        output_dir = os.path.join(my_path, &#34;../output/{}/ID_{}&#34;.format(dataset_name, replication_id))
    else:
        output_dir = options.output_dir

    nb_days = options.window_size
    support_min = options.support_min
    accuracy_min = options.accuracy_min
    Tep = options.tep

    print(&#34;Dataset Name : {}&#34;.format(dataset_name.upper()))
    print(&#34;Replication ID : {}&#34;.format(replication_id))
    print(&#34;Output Directory : {}&#34;.format(output_dir))
    print(&#34;Number of days selected : {}&#34;.format(nb_days))
    print(&#34;Support Minimum : {}&#34;.format(support_min))
    print(&#34;Accuracy Minimum : {}&#34;.format(accuracy_min))
    print(&#34;Tep : {}&#34;.format(Tep))


    dataset = pick_dataset(name=dataset_name, nb_days=nb_days)

    # Find the best case among different tries:

    start_time = t.process_time()
    patterns, patterns_string, data_left = pattern_discovery(data=dataset, Tep=Tep, accuracy_min=0.3,
                                                             support_min=support_min)

    ratio_data_treated = round((1 - len(data_left) / len(dataset)) * 100, 2)

    elapsed_time = dt.timedelta(seconds=round(t.process_time() - start_time, 1))

    print(&#34;\n&#34;)
    print(&#34;###############################&#34;)
    print(&#34;Time to discover the patterns : {}&#34;.format(elapsed_time))
    print(
        &#34;{}% of the {} log_dataset data explained by the patterns discovered&#34;.format(ratio_data_treated, dataset_name))
    print(&#34;##############################&#34;)
    print(&#34;\n&#34;)

    log_filename = output_dir + &#34;/log.txt&#34;
    if not os.path.exists(os.path.dirname(log_filename)):
        try:
            os.makedirs(os.path.dirname(log_filename))
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise

    with open(log_filename, &#39;w+&#39;) as file:
        file.write(&#34;Parameters :\n&#34;)
        file.write(&#34;Support Min : {}\n&#34;.format(support_min))
        file.write(&#34;Accuracy Min : {}\n&#34;.format(accuracy_min))
        file.write(&#34;Tep : {}\n&#34;.format(Tep))
        file.write(&#34;Time to process all the tries : {}\n&#34;.format(elapsed_time))
        file.write(&#34;{}% of the {} log_dataset explained by the patterns discovered\n&#34;.format(ratio_data_treated,
                                                                                             dataset_name))
        file.write(&#34;{} Patterns found\n&#34;.format(len(patterns)))

    # Dump the results in pickle files to re-use them later
    pickle.dump(patterns, open(output_dir + &#34;/patterns.pickle&#34;, &#39;wb&#39;))
    pickle.dump(data_left, open(output_dir + &#34;/data_left.pickle&#34;, &#39;wb&#39;))

    # Write readable results in csv file
    patterns_string.to_csv(output_dir + &#34;/patterns.csv&#34;, sep=&#34;;&#34;, index=False)
    data_left.to_csv(output_dir + &#34;/data_left.csv&#34;, sep=&#34;;&#34;, index=False)

    print(&#34;OUTPUT DIR : {}&#34;.format(output_dir))


def pattern_discovery(data, Tep=30, support_min=2, accuracy_min=0.5,
                      std_max=0.1, tolerance_ratio=2, delta_Tmax_ratio=3, verbose=True, weekly_habits=False):
    &#34;&#34;&#34;
    Implementation of the extended Discovery Algorithm designed by Julie Soulas U{https://hal.archives-ouvertes.fr/tel-01356217/}

    :param data : Starting dataframe, date[datetime] as period_ts_index and 1 column named &#34;label&#34;
    :param Tep : [in Minutes] Maximal time interval between events in an episode occurrence. Should correspond to the maximal duration of the ADLs.
    :param support_min : [greater than 1] Minimal number of occurrences of an episode, for that episode to be considered as frequent.
    :param accuracy_min : [between 0 and 1] Minimal accuracy for a periodicity description to be considered as interesting, and thus factorized
    :param std_max : Standard deviation max for a description (ratio of the period)
    :param tolerance_ratio : [greater than 0] An event expected to happen at time t (with standard deviation sigma) occurs as expected if it occurs in the interval [t - tolerance_ratio*sigma, t + tolerance_ratio*sigma]
    :param delta_Tmax_ratio : If there is a gap &gt; delta_Tmax_ratio * Period between two occurrences of an episode, theoccurrences before and after the gap are split (different validity intervals).
    :return The compressed log_dataset
    &#34;&#34;&#34;
    compressed = True

    final_periodicities = pd.DataFrame(columns=[&#34;Episode&#34;, &#34;Period&#34;, &#34;Description&#34;, &#34;Validity Duration&#34;,
                                                &#34;Start Time&#34;, &#34;End Time&#34;, &#34;Compression Power&#34;, &#34;Accuracy&#34;])
    final_periodicities_string = pd.DataFrame(columns=[&#34;Episode&#34;, &#34;Period&#34;, &#34;Description&#34;, &#34;Validity Duration&#34;,
                                                       &#34;Start Time&#34;, &#34;End Time&#34;, &#34;Compression Power&#34;, &#34;Accuracy&#34;])
    comp_iter = 0

    candidate_periods = [dt.timedelta(days=1), ]

    if weekly_habits:
        candidate_periods.append(dt.timedelta(days=7))
    while compressed:
        comp_iter += 1

        if verbose:
            print(&#34;\n&#34;)
            print(&#34;###############################&#34;)
            print(&#34;#    COMPRESSION NÂ°%d START   #&#34; % comp_iter)
            print(&#34;##############################&#34;)
            print(&#34;\n&#34;)

        compressed = False

        if verbose:
            print(&#34;  Finding frequent episodes candidates...  &#34;.center(100, &#39;*&#39;))

        frequent_episodes = FP_growth.find_frequent_episodes(data, support_min, Tep)

        print(len(frequent_episodes), &#34;candidate episodes found !!&#34;)

        periodicities = {}

        print(&#34; Building candidates episodes periodicities... &#34;.center(100, &#39;*&#39;))

        episode_index = 0
        for episode in frequent_episodes.keys():
            episode_index += 1
            # Build the description of the episode if interesting enough (Accuracy &gt; Accuracy_min)
            description = Candidate_Study.periodicity_search(data, episode,
                                                             delta_Tmax_ratio=delta_Tmax_ratio,
                                                             support_min=support_min,
                                                             std_max=std_max,
                                                             accuracy_min=accuracy_min,
                                                             tolerance_ratio=tolerance_ratio, Tep=Tep,
                                                             candidate_periods=candidate_periods)
            if description is not None:
                # print(&#34;\nInteresting periodicity found for the episode&#34;, episode)
                periodicities[episode] = description

            sys.stdout.write(&#34;\r%.2f %% of episodes treated!!&#34; % (100 * episode_index / len(frequent_episodes)))
            sys.stdout.flush()
        sys.stdout.write(&#34;\n&#34;)

        if len(periodicities) == 0:
            print(&#34;No more intersting periodicities found!!&#34;.center(100, &#39;*&#39;))
            break;
        else:
            print((str(len(periodicities)) + &#34; interesting periodicities found!!&#34;).center(100, &#39;*&#39;))

        print(&#34;Sorting the interesting periodicities by compression power...&#34;.center(100, &#39;*&#39;))
        sorted_episode = sorted(periodicities.keys(), key=lambda x: periodicities[x][&#34;compression_power&#34;], reverse=True)

        print(&#34;Dataset Rewriting&#34;.center(100, &#39;*&#39;))

        factorised_events = pd.DataFrame(columns=[&#34;date&#34;, &#34;label&#34;])

        data_bis = data.copy()  # Data to handle the rewriting
        while True:

            episode = sorted_episode[0]
            periodicity = periodicities[episode]

            if periodicity[&#34;compression_power&#34;] &lt;= 1:
                print(&#34;### COMPRESSION FINISHED : Insufficient compression power reached...&#34;)
                break

            expected_occurrences = periodicity[&#34;expected_occurrences&#34;]

            mini_factorised_events = pd.DataFrame(columns=[&#34;date&#34;, &#34;label&#34;])

            # Find the events corresponding to the expected occurrences
            for index, occurrence in expected_occurrences.iterrows():
                start_date = occurrence[&#34;date&#34;]
                end_date = start_date + dt.timedelta(minutes=Tep)
                mini_data = data_bis.loc[(data_bis.label.isin(episode))
                                         &amp; (data_bis.date &gt;= start_date)
                                         &amp; (data_bis.date &lt; end_date)].copy()
                mini_data.sort_values([&#34;date&#34;], ascending=True, inplace=True)
                mini_data.drop_duplicates([&#34;label&#34;], keep=&#39;first&#39;, inplace=True)
                mini_factorised_events = mini_factorised_events.append(mini_data, ignore_index=True)

            factorised_events = factorised_events.append(mini_factorised_events, ignore_index=True)
            factorised_events.sort_values(by=[&#39;date&#39;], ascending=True, inplace=True)
            count_duplicates = factorised_events.duplicated([&#39;date&#39;, &#39;label&#39;]).sum()
            if count_duplicates != 0:
                # Current periodicity involves events in factorized_events
                print(&#34;### COMPRESSION FINISHED : Overlapping factorized events reached...&#34;)
                break

            # Factorize DATA
            data = pd.concat([data, mini_factorised_events], sort=True).drop_duplicates(keep=False)

            # Add missing events
            # FIXME : Do we add missing events or not?
            if False:
                events_to_add = find_missing_events(data_bis, episode, expected_occurrences,
                                                    periodicity[&#34;description&#34;], periodicity[&#34;period&#34;], tolerance_ratio)
                data = pd.concat([data, events_to_add]).drop_duplicates(keep=False)

            data.reset_index(inplace=True, drop=True)

            # Add the periodicity to the results
            natural_periodicity = Candidate_Study.translate_description(periodicity)

            final_periodicities.loc[len(final_periodicities)] = [episode, periodicity[&#34;period&#34;],
                                                                 periodicity[&#34;description&#34;],
                                                                 periodicity[&#34;delta_t&#34;][1] - periodicity[&#34;delta_t&#34;][0],
                                                                 periodicity[&#34;delta_t&#34;][0],
                                                                 periodicity[&#34;delta_t&#34;][1],
                                                                 periodicity[&#34;compression_power&#34;],
                                                                 periodicity[&#34;accuracy&#34;]]

            final_periodicities_string.loc[len(final_periodicities_string)] = [episode, natural_periodicity[&#34;period&#34;],
                                                                               natural_periodicity[&#34;description&#34;],
                                                                               natural_periodicity[&#34;validity duration&#34;],
                                                                               natural_periodicity[&#34;delta_t&#34;][0],
                                                                               natural_periodicity[&#34;delta_t&#34;][1],
                                                                               natural_periodicity[&#34;compression_power&#34;],
                                                                               natural_periodicity[&#34;accuracy&#34;]]

            # Remove periodicity from the list
            sorted_episode = sorted_episode[1:]
            if len(sorted_episode) == 0:
                print(&#34;### COMPRESSION FINISHED : No more frequent episodes...&#34;)
                break;

            compressed = True

    return final_periodicities, final_periodicities_string, data


def find_missing_events(data, episode, occurrences, description, period, tolerance_ratio):
    &#34;&#34;&#34;
    Find missing occurrences of the episode description in the original log_dataset
    :return the missing events
    &#34;&#34;&#34;

    data = data.loc[data.label.isin(episode)]

    missing_events_df = pd.DataFrame(columns=[&#34;date&#34;, &#34;label&#34;])

    occ_start_time = occurrences.date.min().to_pydatetime()
    start_period_date = occ_start_time + dt.timedelta(
        seconds=(period.total_seconds() - Candidate_Study.modulo_datetime(occ_start_time, period)))

    occ_end_time = occurrences.date.max().to_pydatetime()
    end_period_date = occ_end_time - dt.timedelta(
        seconds=Candidate_Study.modulo_datetime(occ_end_time, period))

    # Deal with the periods
    current_period_start_date = start_period_date
    while current_period_start_date &lt; end_period_date:
        current_period_end_date = current_period_start_date + period
        for mu, sigma in description.items():
            comp_start_date = current_period_start_date + dt.timedelta(seconds=(mu - tolerance_ratio * sigma))
            comp_end_date = current_period_start_date + dt.timedelta(seconds=(mu + tolerance_ratio * sigma))

            occurrence_happenned = len(
                occurrences.loc[(occurrences.date &gt;= comp_start_date) &amp; (occurrences.date &lt;= comp_end_date)]) &gt; 0

            if occurrence_happenned:
                continue

            # If not happenned fill the missing events
            present_events = set(
                data.loc[(data.date &gt;= comp_start_date) &amp; (data.date &lt;= comp_end_date), &#34;label&#34;].values)
            intersection = present_events.intersection(episode)
            missing_events = set(episode) - intersection

            for event in intersection:
                event_date = data.loc[
                    (data.date &gt;= comp_start_date) &amp; (data.date &lt;= comp_end_date) &amp; (data.label == event)].date.min()
                missing_events_df.loc[len(missing_events_df)] = [event_date, event]
            for event in missing_events:
                ts = int(mu + sigma * np.random.randn())
                event_date = current_period_start_date + dt.timedelta(seconds=ts)
                missing_events_df.loc[len(missing_events_df)] = [event_date, &#34;MISSING &#34; + event]

        current_period_start_date = current_period_end_date

    # Now the bord effects
    for mu, sigma in description.items():
        if mu &lt; Candidate_Study.modulo_datetime(occ_start_time, period):
            continue
        if mu &gt; Candidate_Study.modulo_datetime(occ_end_time, period):
            continue

        comp_start_date = current_period_start_date + dt.timedelta(seconds=(mu - tolerance_ratio * sigma))
        comp_end_date = current_period_start_date + dt.timedelta(seconds=(mu + tolerance_ratio * sigma))

        occurrence_happenned = len(
            occurrences.loc[(occurrences.date &gt;= comp_start_date) &amp; (occurrences.date &lt;= comp_end_date)]) &gt; 0

        if occurrence_happenned:
            continue

        # If not happenned fill the missing events
        present_events = set(data.loc[(data.date &gt;= comp_start_date) &amp; (data.date &lt;= comp_end_date), &#34;label&#34;].values)
        missing_events = set(episode) - present_events.intersection(episode)

        for event in missing_events:
            ts = int(mu + sigma * np.random.randn())
            event_date = current_period_start_date + dt.timedelta(seconds=ts)
            missing_events_df.loc[len(missing_events_df)] = [event_date, &#34;MISSING &#34; + event]

    return missing_events_df





if __name__ == &#34;__main__&#34;:
    main()</code></pre>
            </details>
        </section>
        <section>
        </section>
        <section>
        </section>
        <section>
            <h2 class="section-title" id="header-functions">Functions</h2>
            <dl>
                <dt id="Pattern_Mining.Pattern_Discovery.find_missing_events"><code class="name flex">
                    <span>def <span class="ident">find_missing_events</span></span>(<span>data, episode, occurrences, description, period, tolerance_ratio)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>Find missing occurrences of the episode description in the original log_dataset
                        :return the missing events</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def find_missing_events(data, episode, occurrences, description, period, tolerance_ratio):
    &#34;&#34;&#34;
    Find missing occurrences of the episode description in the original log_dataset
    :return the missing events
    &#34;&#34;&#34;

    data = data.loc[data.label.isin(episode)]

    missing_events_df = pd.DataFrame(columns=[&#34;date&#34;, &#34;label&#34;])

    occ_start_time = occurrences.date.min().to_pydatetime()
    start_period_date = occ_start_time + dt.timedelta(
        seconds=(period.total_seconds() - Candidate_Study.modulo_datetime(occ_start_time, period)))

    occ_end_time = occurrences.date.max().to_pydatetime()
    end_period_date = occ_end_time - dt.timedelta(
        seconds=Candidate_Study.modulo_datetime(occ_end_time, period))

    # Deal with the periods
    current_period_start_date = start_period_date
    while current_period_start_date &lt; end_period_date:
        current_period_end_date = current_period_start_date + period
        for mu, sigma in description.items():
            comp_start_date = current_period_start_date + dt.timedelta(seconds=(mu - tolerance_ratio * sigma))
            comp_end_date = current_period_start_date + dt.timedelta(seconds=(mu + tolerance_ratio * sigma))

            occurrence_happenned = len(
                occurrences.loc[(occurrences.date &gt;= comp_start_date) &amp; (occurrences.date &lt;= comp_end_date)]) &gt; 0

            if occurrence_happenned:
                continue

            # If not happenned fill the missing events
            present_events = set(
                data.loc[(data.date &gt;= comp_start_date) &amp; (data.date &lt;= comp_end_date), &#34;label&#34;].values)
            intersection = present_events.intersection(episode)
            missing_events = set(episode) - intersection

            for event in intersection:
                event_date = data.loc[
                    (data.date &gt;= comp_start_date) &amp; (data.date &lt;= comp_end_date) &amp; (data.label == event)].date.min()
                missing_events_df.loc[len(missing_events_df)] = [event_date, event]
            for event in missing_events:
                ts = int(mu + sigma * np.random.randn())
                event_date = current_period_start_date + dt.timedelta(seconds=ts)
                missing_events_df.loc[len(missing_events_df)] = [event_date, &#34;MISSING &#34; + event]

        current_period_start_date = current_period_end_date

    # Now the bord effects
    for mu, sigma in description.items():
        if mu &lt; Candidate_Study.modulo_datetime(occ_start_time, period):
            continue
        if mu &gt; Candidate_Study.modulo_datetime(occ_end_time, period):
            continue

        comp_start_date = current_period_start_date + dt.timedelta(seconds=(mu - tolerance_ratio * sigma))
        comp_end_date = current_period_start_date + dt.timedelta(seconds=(mu + tolerance_ratio * sigma))

        occurrence_happenned = len(
            occurrences.loc[(occurrences.date &gt;= comp_start_date) &amp; (occurrences.date &lt;= comp_end_date)]) &gt; 0

        if occurrence_happenned:
            continue

        # If not happenned fill the missing events
        present_events = set(data.loc[(data.date &gt;= comp_start_date) &amp; (data.date &lt;= comp_end_date), &#34;label&#34;].values)
        missing_events = set(episode) - present_events.intersection(episode)

        for event in missing_events:
            ts = int(mu + sigma * np.random.randn())
            event_date = current_period_start_date + dt.timedelta(seconds=ts)
            missing_events_df.loc[len(missing_events_df)] = [event_date, &#34;MISSING &#34; + event]

    return missing_events_df</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Pattern_Discovery.main"><code class="name flex">
                    <span>def <span class="ident">main</span></span>(<span>)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>The dataframe should have 1 period_ts_index (date as datetime) and 1 feature
                        (label)</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def main():
    ######################
    # DATA PREPROCESSING #
    ######################

    &#34;&#34;&#34;
    The dataframe should have 1 period_ts_index (date as datetime) and 1 feature (label)
    &#34;&#34;&#34;

    parser = OptionParser(usage=&#39;Usage of the Pattern Discovery algorihtm: %prog &lt;options&gt;&#39;)
    parser.add_option(&#39;-n&#39;, &#39;--dataset_name&#39;, help=&#39;Name of the Input event log&#39;, dest=&#39;dataset_name&#39;, action=&#39;store&#39;,
                      type=&#39;string&#39;)
    parser.add_option(&#39;-r&#39;, &#39;--replication&#39;, help=&#39;Replication Identifier&#39;, dest=&#39;replication&#39;, action=&#39;store&#39;,
                      type=int, default=0)
    parser.add_option(&#39;-o&#39;, &#39;--output_dir&#39;, help=&#39;Output directory&#39;, dest=&#39;output_dir&#39;, action=&#39;store&#39;, type=&#39;string&#39;)
    parser.add_option(&#39;-w&#39;, &#39;--window_size&#39;, help=&#39;Number of the days used&#39;, dest=&#39;window_size&#39;, action=&#39;store&#39;,
                      type=int, default=-1)
    parser.add_option(&#39;-s&#39;, &#39;--support&#39;, help=&#39;Minimum number of occurrences of a pattern&#39;, dest=&#39;support_min&#39;,
                      action=&#39;store&#39;, type=int, default=3)
    parser.add_option(&#39;-a&#39;, &#39;--accuracy_min&#39;, help=&#39;Accuracy min of a pattern&#39;, dest=&#39;accuracy_min&#39;, action=&#39;store&#39;,
                      type=float, default=0.5)
    parser.add_option(&#39;-t&#39;, &#39;--tep&#39;, help=&#39;Duration max of an occurrence (in minutes)&#39;, dest=&#39;tep&#39;, action=&#39;store&#39;,
                      type=int, default=30)

    (options, args) = parser.parse_args()
    # Mandatory Options
    if options.dataset_name is None:
        print(&#34;The name of the Input event log is missing\n&#34;)
        parser.print_help()
        exit(-1)

    dataset_name = options.dataset_name
    replication_id = options.replication

    if options.output_dir is None:
        my_path = os.path.abspath(os.path.dirname(__file__))
        output_dir = os.path.join(my_path, &#34;../output/{}/ID_{}&#34;.format(dataset_name, replication_id))
    else:
        output_dir = options.output_dir

    nb_days = options.window_size
    support_min = options.support_min
    accuracy_min = options.accuracy_min
    Tep = options.tep

    print(&#34;Dataset Name : {}&#34;.format(dataset_name.upper()))
    print(&#34;Replication ID : {}&#34;.format(replication_id))
    print(&#34;Output Directory : {}&#34;.format(output_dir))
    print(&#34;Number of days selected : {}&#34;.format(nb_days))
    print(&#34;Support Minimum : {}&#34;.format(support_min))
    print(&#34;Accuracy Minimum : {}&#34;.format(accuracy_min))
    print(&#34;Tep : {}&#34;.format(Tep))


    dataset = pick_dataset(name=dataset_name, nb_days=nb_days)

    # Find the best case among different tries:

    start_time = t.process_time()
    patterns, patterns_string, data_left = pattern_discovery(data=dataset, Tep=Tep, accuracy_min=0.3,
                                                             support_min=support_min)

    ratio_data_treated = round((1 - len(data_left) / len(dataset)) * 100, 2)

    elapsed_time = dt.timedelta(seconds=round(t.process_time() - start_time, 1))

    print(&#34;\n&#34;)
    print(&#34;###############################&#34;)
    print(&#34;Time to discover the patterns : {}&#34;.format(elapsed_time))
    print(
        &#34;{}% of the {} log_dataset data explained by the patterns discovered&#34;.format(ratio_data_treated, dataset_name))
    print(&#34;##############################&#34;)
    print(&#34;\n&#34;)

    log_filename = output_dir + &#34;/log.txt&#34;
    if not os.path.exists(os.path.dirname(log_filename)):
        try:
            os.makedirs(os.path.dirname(log_filename))
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise

    with open(log_filename, &#39;w+&#39;) as file:
        file.write(&#34;Parameters :\n&#34;)
        file.write(&#34;Support Min : {}\n&#34;.format(support_min))
        file.write(&#34;Accuracy Min : {}\n&#34;.format(accuracy_min))
        file.write(&#34;Tep : {}\n&#34;.format(Tep))
        file.write(&#34;Time to process all the tries : {}\n&#34;.format(elapsed_time))
        file.write(&#34;{}% of the {} log_dataset explained by the patterns discovered\n&#34;.format(ratio_data_treated,
                                                                                             dataset_name))
        file.write(&#34;{} Patterns found\n&#34;.format(len(patterns)))

    # Dump the results in pickle files to re-use them later
    pickle.dump(patterns, open(output_dir + &#34;/patterns.pickle&#34;, &#39;wb&#39;))
    pickle.dump(data_left, open(output_dir + &#34;/data_left.pickle&#34;, &#39;wb&#39;))

    # Write readable results in csv file
    patterns_string.to_csv(output_dir + &#34;/patterns.csv&#34;, sep=&#34;;&#34;, index=False)
    data_left.to_csv(output_dir + &#34;/data_left.csv&#34;, sep=&#34;;&#34;, index=False)

    print(&#34;OUTPUT DIR : {}&#34;.format(output_dir))</code></pre>
                    </details>
                </dd>
                <dt id="Pattern_Mining.Pattern_Discovery.pattern_discovery"><code class="name flex">
                    <span>def <span class="ident">pattern_discovery</span></span>(<span>data, Tep=30, support_min=2, accuracy_min=0.5, std_max=0.1, tolerance_ratio=2, delta_Tmax_ratio=3, verbose=True, weekly_habits=False)</span>
                </code></dt>
                <dd>
                    <div class="desc"><p>Implementation of the extended Discovery Algorithm designed by Julie Soulas
                        U{<a href="https://hal.archives-ouvertes.fr/tel-01356217/}">https://hal.archives-ouvertes.fr/tel-01356217/}</a>
                    </p>
                        <p>:param data : Starting dataframe, date[datetime] as period_ts_index and 1 column named
                            "label"
                            :param Tep : [in Minutes] Maximal time interval between events in an episode occurrence.
                            Should correspond to the maximal duration of the ADLs.
                            :param support_min : [greater than 1] Minimal number of occurrences of an episode, for that
                            episode to be considered as frequent.
                            :param accuracy_min : [between 0 and 1] Minimal accuracy for a periodicity description to be
                            considered as interesting, and thus factorized
                            :param std_max : Standard deviation max for a description (ratio of the period)
                            :param tolerance_ratio : [greater than 0] An event expected to happen at time t (with
                            standard deviation sigma) occurs as expected if it occurs in the interval [t -
                            tolerance_ratio<em>sigma, t + tolerance_ratio</em>sigma]
                            :param delta_Tmax_ratio : If there is a gap &gt; delta_Tmax_ratio * Period between two
                            occurrences of an episode, theoccurrences before and after the gap are split (different
                            validity intervals).
                            :return The compressed log_dataset</p></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def pattern_discovery(data, Tep=30, support_min=2, accuracy_min=0.5,
                      std_max=0.1, tolerance_ratio=2, delta_Tmax_ratio=3, verbose=True, weekly_habits=False):
    &#34;&#34;&#34;
    Implementation of the extended Discovery Algorithm designed by Julie Soulas U{https://hal.archives-ouvertes.fr/tel-01356217/}

    :param data : Starting dataframe, date[datetime] as period_ts_index and 1 column named &#34;label&#34;
    :param Tep : [in Minutes] Maximal time interval between events in an episode occurrence. Should correspond to the maximal duration of the ADLs.
    :param support_min : [greater than 1] Minimal number of occurrences of an episode, for that episode to be considered as frequent.
    :param accuracy_min : [between 0 and 1] Minimal accuracy for a periodicity description to be considered as interesting, and thus factorized
    :param std_max : Standard deviation max for a description (ratio of the period)
    :param tolerance_ratio : [greater than 0] An event expected to happen at time t (with standard deviation sigma) occurs as expected if it occurs in the interval [t - tolerance_ratio*sigma, t + tolerance_ratio*sigma]
    :param delta_Tmax_ratio : If there is a gap &gt; delta_Tmax_ratio * Period between two occurrences of an episode, theoccurrences before and after the gap are split (different validity intervals).
    :return The compressed log_dataset
    &#34;&#34;&#34;
    compressed = True

    final_periodicities = pd.DataFrame(columns=[&#34;Episode&#34;, &#34;Period&#34;, &#34;Description&#34;, &#34;Validity Duration&#34;,
                                                &#34;Start Time&#34;, &#34;End Time&#34;, &#34;Compression Power&#34;, &#34;Accuracy&#34;])
    final_periodicities_string = pd.DataFrame(columns=[&#34;Episode&#34;, &#34;Period&#34;, &#34;Description&#34;, &#34;Validity Duration&#34;,
                                                       &#34;Start Time&#34;, &#34;End Time&#34;, &#34;Compression Power&#34;, &#34;Accuracy&#34;])
    comp_iter = 0

    candidate_periods = [dt.timedelta(days=1), ]

    if weekly_habits:
        candidate_periods.append(dt.timedelta(days=7))
    while compressed:
        comp_iter += 1

        if verbose:
            print(&#34;\n&#34;)
            print(&#34;###############################&#34;)
            print(&#34;#    COMPRESSION NÂ°%d START   #&#34; % comp_iter)
            print(&#34;##############################&#34;)
            print(&#34;\n&#34;)

        compressed = False

        if verbose:
            print(&#34;  Finding frequent episodes candidates...  &#34;.center(100, &#39;*&#39;))

        frequent_episodes = FP_growth.find_frequent_episodes(data, support_min, Tep)

        print(len(frequent_episodes), &#34;candidate episodes found !!&#34;)

        periodicities = {}

        print(&#34; Building candidates episodes periodicities... &#34;.center(100, &#39;*&#39;))

        episode_index = 0
        for episode in frequent_episodes.keys():
            episode_index += 1
            # Build the description of the episode if interesting enough (Accuracy &gt; Accuracy_min)
            description = Candidate_Study.periodicity_search(data, episode,
                                                             delta_Tmax_ratio=delta_Tmax_ratio,
                                                             support_min=support_min,
                                                             std_max=std_max,
                                                             accuracy_min=accuracy_min,
                                                             tolerance_ratio=tolerance_ratio, Tep=Tep,
                                                             candidate_periods=candidate_periods)
            if description is not None:
                # print(&#34;\nInteresting periodicity found for the episode&#34;, episode)
                periodicities[episode] = description

            sys.stdout.write(&#34;\r%.2f %% of episodes treated!!&#34; % (100 * episode_index / len(frequent_episodes)))
            sys.stdout.flush()
        sys.stdout.write(&#34;\n&#34;)

        if len(periodicities) == 0:
            print(&#34;No more intersting periodicities found!!&#34;.center(100, &#39;*&#39;))
            break;
        else:
            print((str(len(periodicities)) + &#34; interesting periodicities found!!&#34;).center(100, &#39;*&#39;))

        print(&#34;Sorting the interesting periodicities by compression power...&#34;.center(100, &#39;*&#39;))
        sorted_episode = sorted(periodicities.keys(), key=lambda x: periodicities[x][&#34;compression_power&#34;], reverse=True)

        print(&#34;Dataset Rewriting&#34;.center(100, &#39;*&#39;))

        factorised_events = pd.DataFrame(columns=[&#34;date&#34;, &#34;label&#34;])

        data_bis = data.copy()  # Data to handle the rewriting
        while True:

            episode = sorted_episode[0]
            periodicity = periodicities[episode]

            if periodicity[&#34;compression_power&#34;] &lt;= 1:
                print(&#34;### COMPRESSION FINISHED : Insufficient compression power reached...&#34;)
                break

            expected_occurrences = periodicity[&#34;expected_occurrences&#34;]

            mini_factorised_events = pd.DataFrame(columns=[&#34;date&#34;, &#34;label&#34;])

            # Find the events corresponding to the expected occurrences
            for index, occurrence in expected_occurrences.iterrows():
                start_date = occurrence[&#34;date&#34;]
                end_date = start_date + dt.timedelta(minutes=Tep)
                mini_data = data_bis.loc[(data_bis.label.isin(episode))
                                         &amp; (data_bis.date &gt;= start_date)
                                         &amp; (data_bis.date &lt; end_date)].copy()
                mini_data.sort_values([&#34;date&#34;], ascending=True, inplace=True)
                mini_data.drop_duplicates([&#34;label&#34;], keep=&#39;first&#39;, inplace=True)
                mini_factorised_events = mini_factorised_events.append(mini_data, ignore_index=True)

            factorised_events = factorised_events.append(mini_factorised_events, ignore_index=True)
            factorised_events.sort_values(by=[&#39;date&#39;], ascending=True, inplace=True)
            count_duplicates = factorised_events.duplicated([&#39;date&#39;, &#39;label&#39;]).sum()
            if count_duplicates != 0:
                # Current periodicity involves events in factorized_events
                print(&#34;### COMPRESSION FINISHED : Overlapping factorized events reached...&#34;)
                break

            # Factorize DATA
            data = pd.concat([data, mini_factorised_events], sort=True).drop_duplicates(keep=False)

            # Add missing events
            # FIXME : Do we add missing events or not?
            if False:
                events_to_add = find_missing_events(data_bis, episode, expected_occurrences,
                                                    periodicity[&#34;description&#34;], periodicity[&#34;period&#34;], tolerance_ratio)
                data = pd.concat([data, events_to_add]).drop_duplicates(keep=False)

            data.reset_index(inplace=True, drop=True)

            # Add the periodicity to the results
            natural_periodicity = Candidate_Study.translate_description(periodicity)

            final_periodicities.loc[len(final_periodicities)] = [episode, periodicity[&#34;period&#34;],
                                                                 periodicity[&#34;description&#34;],
                                                                 periodicity[&#34;delta_t&#34;][1] - periodicity[&#34;delta_t&#34;][0],
                                                                 periodicity[&#34;delta_t&#34;][0],
                                                                 periodicity[&#34;delta_t&#34;][1],
                                                                 periodicity[&#34;compression_power&#34;],
                                                                 periodicity[&#34;accuracy&#34;]]

            final_periodicities_string.loc[len(final_periodicities_string)] = [episode, natural_periodicity[&#34;period&#34;],
                                                                               natural_periodicity[&#34;description&#34;],
                                                                               natural_periodicity[&#34;validity duration&#34;],
                                                                               natural_periodicity[&#34;delta_t&#34;][0],
                                                                               natural_periodicity[&#34;delta_t&#34;][1],
                                                                               natural_periodicity[&#34;compression_power&#34;],
                                                                               natural_periodicity[&#34;accuracy&#34;]]

            # Remove periodicity from the list
            sorted_episode = sorted_episode[1:]
            if len(sorted_episode) == 0:
                print(&#34;### COMPRESSION FINISHED : No more frequent episodes...&#34;)
                break;

            compressed = True

    return final_periodicities, final_periodicities_string, data</code></pre>
                    </details>
                </dd>
            </dl>
        </section>
        <section>
        </section>
    </article>
    <nav id="sidebar">
        <h1>Index</h1>
        <div class="toc">
            <ul></ul>
        </div>
        <ul id="index">
            <li><h3>Super-module</h3>
                <ul>
                    <li><code><a href="index.html" title="Pattern_Mining">Pattern_Mining</a></code></li>
                </ul>
            </li>
            <li><h3><a href="#header-functions">Functions</a></h3>
                <ul class="">
                    <li><code><a href="#Pattern_Mining.Pattern_Discovery.find_missing_events"
                                 title="Pattern_Mining.Pattern_Discovery.find_missing_events">find_missing_events</a></code>
                    </li>
                    <li><code><a href="#Pattern_Mining.Pattern_Discovery.main"
                                 title="Pattern_Mining.Pattern_Discovery.main">main</a></code></li>
                    <li><code><a href="#Pattern_Mining.Pattern_Discovery.pattern_discovery"
                                 title="Pattern_Mining.Pattern_Discovery.pattern_discovery">pattern_discovery</a></code>
                    </li>
                </ul>
            </li>
        </ul>
    </nav>
</main>
<footer id="footer">
    <p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>