<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1, minimum-scale=1" name="viewport"/>
    <meta content="pdoc 0.9.2" name="generator"/>
    <title>Time_Series.LSTM API documentation</title>
    <meta content="" name="description"/>
    <link as="style" crossorigin href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css"
          integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" rel="preload stylesheet">
    <link as="style" crossorigin
          href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css"
          integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" rel="preload stylesheet">
    <link as="style" crossorigin href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css"
          rel="stylesheet preload">
    <style>
        :root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}
    </style>
    <style media="screen and (min-width: 700px)">
        @media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}
    </style>
    <style media="print">
        @media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}
    </style>
    <script crossorigin defer integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8="
            src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
    <script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
    <article id="content">
        <header>
            <h1 class="title">Module <code>Time_Series.LSTM</code></h1>
        </header>
        <section id="section-intro">
            <details class="source">
                <summary>
                    <span>Expand source code</span>
                </summary>
                <pre><code class="python">import math
import pickle

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from keras.layers import Dense
from keras.layers import LSTM
from keras.models import Sequential
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler

sns.set_style(&#39;darkgrid&#39;)
np.random.seed(7)


def main():
    dataset_name = &#34;aruba&#34;

    # episode = (&#39;bed_to_toilet&#39;, &#39;sleeping&#39;)
    episode = (&#39;bed_to_toilet&#39;, &#39;sleeping&#39;)

    output = &#34;C:/Users/cyriac.azefack/Workspace/Frailty_Box/output/{}/Activity_{}/&#34;.format(dataset_name, episode)

    daily_profile = pickle.load(open(output + &#34;daily_profile.pkl&#34;, &#34;rb&#34;))

    duration_distrib = pickle.load(open(output + &#34;durations_distrib.pkl&#34;, &#34;rb&#34;))

    occurrence_order = pickle.load(open(output + &#34;execution_order.pkl&#34;, &#34;rb&#34;))

    inter_events = pickle.load(open(output + &#34;inter_events.pkl&#34;, &#34;rb&#34;))

    names = []
    # data = duration_distrib[episode[0]][&#39;std&#39;].values
    # data = inter_events[&#34;lambda&#34;].values
    dataset = duration_distrib[episode[0]].drop(columns=[&#39;tw_id&#39;]).values

    dataset = inter_events
    names.append(&#39;lambda_inter_events&#39;)

    for label in episode:
        data = duration_distrib[label]
        dataset = dataset.join(data.set_index(&#39;tw_id&#39;), on=&#39;tw_id&#39;, rsuffix=&#39;_&#39; + label)
        names.append(&#39;mean_{}&#39;.format(label))
        names.append(&#39;std_{}&#39;.format(label))

    dataset = dataset.join(occurrence_order.set_index(&#39;tw_id&#39;), on=&#39;tw_id&#39;)
    names += list(occurrence_order.columns[1:])

    dataset.drop(columns=[&#39;tw_id&#39;], inplace=True)

    dataset.columns = names

    n_features = len(names)
    look_back = 1
    train_ratio = 0.8
    test_ratio = 0.1

    # plt.plot(log_dataset)
    # plt.show()

    # normalize the log_dataset
    scaler = MinMaxScaler(feature_range=(0, 1))
    dataset = scaler.fit_transform(dataset)

    # split into train and test sets

    train, test, validate = np.split(dataset,
                                     [int(train_ratio * len(dataset)), int((train_ratio + test_ratio) * len(dataset))])
    # train_size = int(len(log_dataset) * train_ratio)
    # test_size = len(log_dataset) - train_size
    # train, test = log_dataset[0:train_size, :], log_dataset[train_size:len(log_dataset), :]

    print(&#39;Original log_dataset&#39;, train.shape)

    # reshape into X=t and Y=t+1
    trainX, trainY = create_lookback_dataset(train, look_back)
    testX, testY = create_lookback_dataset(test, look_back)
    validateX, validateY = create_lookback_dataset(validate, look_back)

    print(trainX.shape, trainY.shape)

    # reshape input to be [samples, time steps, features]
    # trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
    # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
    # validateX = np.reshape(validateX, (validateX.shape[0], 1, validateX.shape[1]))



    # create and fit the LSTM network
    model = Sequential()
    model.add(LSTM(20, input_shape=(trainX.shape[1], trainX.shape[2])))
    model.add(Dense(n_features))
    model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;)
    history = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(testX, testY), verbose=2)

    # make predictions
    trainPredict = model.predict(trainX)
    testPredict = model.predict(testX)
    validatePredict = model.predict(validateX)

    # invert predictions
    trainPredict = scaler.inverse_transform(trainPredict)
    trainY = scaler.inverse_transform(trainY)
    testPredict = scaler.inverse_transform(testPredict)
    testY = scaler.inverse_transform(testY)
    validatePredict = scaler.inverse_transform(validatePredict)
    validateY = scaler.inverse_transform(validateY)

    plt.figure()
    plt.plot(history.history[&#39;loss&#39;], label=&#39;train&#39;)
    plt.plot(history.history[&#39;val_loss&#39;], label=&#39;test&#39;)
    plt.title(&#39;Entropy loss&#39;)
    plt.legend()
    plt.show()

    # calculate root mean squared
    for i in range(len(names)):
        trainRMSE = math.sqrt(mean_squared_error(trainY[:, i], trainPredict[:, i]))
        print(&#39;Train Percentage of relative error: {:.2f}&#39;.format(trainRMSE))
        testRMSE = math.sqrt(mean_squared_error(testY[:, i], testPredict[:, i]))
        print(&#39;Test Percentage of relative error: {:.2f}&#39;.format(testRMSE))
        validateRMSE = math.sqrt(mean_squared_error(validateY[:, i], validatePredict[:, i]))
        print(&#39;Test Percentage of relative error: {:.2f}&#39;.format(validateRMSE))

        # shift train predictions for plotting
        trainPredictPlot = np.empty_like(dataset[:, i])
        trainPredictPlot[:] = np.nan
        trainPredictPlot[look_back:len(trainPredict) + look_back] = trainPredict[:, i]

        # shift test predictions for plotting
        testPredictPlot = np.empty_like(dataset[:, i])
        testPredictPlot[:] = np.nan
        testPredictPlot[
        len(trainPredict) + (look_back * 2) + 1:len(dataset) - len(validatePredict) - look_back - 2] = testPredict[:, i]

        # shift Validation predictions for plotting
        validatePredictPlot = np.empty_like(dataset[:, i])
        validatePredictPlot[:] = np.nan
        validatePredictPlot[-len(validatePredict):] = validatePredict[:, i]

        # plot baseline and predictions
        plt.figure()
        plt.plot(scaler.inverse_transform(dataset)[:, i], label=&#39;Original&#39;)
        plt.plot(trainPredictPlot, label=&#39;Train Forecast&#39;)
        plt.plot(testPredictPlot, label=&#39;Test Forecast&#39;)
        plt.plot(validatePredictPlot, label=&#39;Validation Forecast&#39;)
        plt.title(&#39;{}\nTrain Error : {:.3f}\nTest Error : {:.3f}&#39;.format(names[i], trainRMSE, testRMSE))

    plt.show()


# convert an array of values into a log_dataset distance_matrix
def create_lookback_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])
    return np.asarray(dataX), np.asarray(dataY)


if __name__ == &#34;__main__&#34;:
    main()</code></pre>
            </details>
        </section>
        <section>
        </section>
        <section>
        </section>
        <section>
            <h2 class="section-title" id="header-functions">Functions</h2>
            <dl>
                <dt id="Time_Series.LSTM.create_lookback_dataset"><code class="name flex">
                    <span>def <span
                            class="ident">create_lookback_dataset</span></span>(<span>dataset, look_back=1)</span>
                </code></dt>
                <dd>
                    <div class="desc"></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def create_lookback_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])
    return np.asarray(dataX), np.asarray(dataY)</code></pre>
                    </details>
                </dd>
                <dt id="Time_Series.LSTM.main"><code class="name flex">
                    <span>def <span class="ident">main</span></span>(<span>)</span>
                </code></dt>
                <dd>
                    <div class="desc"></div>
                    <details class="source">
                        <summary>
                            <span>Expand source code</span>
                        </summary>
                        <pre><code class="python">def main():
    dataset_name = &#34;aruba&#34;

    # episode = (&#39;bed_to_toilet&#39;, &#39;sleeping&#39;)
    episode = (&#39;bed_to_toilet&#39;, &#39;sleeping&#39;)

    output = &#34;C:/Users/cyriac.azefack/Workspace/Frailty_Box/output/{}/Activity_{}/&#34;.format(dataset_name, episode)

    daily_profile = pickle.load(open(output + &#34;daily_profile.pkl&#34;, &#34;rb&#34;))

    duration_distrib = pickle.load(open(output + &#34;durations_distrib.pkl&#34;, &#34;rb&#34;))

    occurrence_order = pickle.load(open(output + &#34;execution_order.pkl&#34;, &#34;rb&#34;))

    inter_events = pickle.load(open(output + &#34;inter_events.pkl&#34;, &#34;rb&#34;))

    names = []
    # data = duration_distrib[episode[0]][&#39;std&#39;].values
    # data = inter_events[&#34;lambda&#34;].values
    dataset = duration_distrib[episode[0]].drop(columns=[&#39;tw_id&#39;]).values

    dataset = inter_events
    names.append(&#39;lambda_inter_events&#39;)

    for label in episode:
        data = duration_distrib[label]
        dataset = dataset.join(data.set_index(&#39;tw_id&#39;), on=&#39;tw_id&#39;, rsuffix=&#39;_&#39; + label)
        names.append(&#39;mean_{}&#39;.format(label))
        names.append(&#39;std_{}&#39;.format(label))

    dataset = dataset.join(occurrence_order.set_index(&#39;tw_id&#39;), on=&#39;tw_id&#39;)
    names += list(occurrence_order.columns[1:])

    dataset.drop(columns=[&#39;tw_id&#39;], inplace=True)

    dataset.columns = names

    n_features = len(names)
    look_back = 1
    train_ratio = 0.8
    test_ratio = 0.1

    # plt.plot(log_dataset)
    # plt.show()

    # normalize the log_dataset
    scaler = MinMaxScaler(feature_range=(0, 1))
    dataset = scaler.fit_transform(dataset)

    # split into train and test sets

    train, test, validate = np.split(dataset,
                                     [int(train_ratio * len(dataset)), int((train_ratio + test_ratio) * len(dataset))])
    # train_size = int(len(log_dataset) * train_ratio)
    # test_size = len(log_dataset) - train_size
    # train, test = log_dataset[0:train_size, :], log_dataset[train_size:len(log_dataset), :]

    print(&#39;Original log_dataset&#39;, train.shape)

    # reshape into X=t and Y=t+1
    trainX, trainY = create_lookback_dataset(train, look_back)
    testX, testY = create_lookback_dataset(test, look_back)
    validateX, validateY = create_lookback_dataset(validate, look_back)

    print(trainX.shape, trainY.shape)

    # reshape input to be [samples, time steps, features]
    # trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
    # testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
    # validateX = np.reshape(validateX, (validateX.shape[0], 1, validateX.shape[1]))



    # create and fit the LSTM network
    model = Sequential()
    model.add(LSTM(20, input_shape=(trainX.shape[1], trainX.shape[2])))
    model.add(Dense(n_features))
    model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;adam&#39;)
    history = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(testX, testY), verbose=2)

    # make predictions
    trainPredict = model.predict(trainX)
    testPredict = model.predict(testX)
    validatePredict = model.predict(validateX)

    # invert predictions
    trainPredict = scaler.inverse_transform(trainPredict)
    trainY = scaler.inverse_transform(trainY)
    testPredict = scaler.inverse_transform(testPredict)
    testY = scaler.inverse_transform(testY)
    validatePredict = scaler.inverse_transform(validatePredict)
    validateY = scaler.inverse_transform(validateY)

    plt.figure()
    plt.plot(history.history[&#39;loss&#39;], label=&#39;train&#39;)
    plt.plot(history.history[&#39;val_loss&#39;], label=&#39;test&#39;)
    plt.title(&#39;Entropy loss&#39;)
    plt.legend()
    plt.show()

    # calculate root mean squared
    for i in range(len(names)):
        trainRMSE = math.sqrt(mean_squared_error(trainY[:, i], trainPredict[:, i]))
        print(&#39;Train Percentage of relative error: {:.2f}&#39;.format(trainRMSE))
        testRMSE = math.sqrt(mean_squared_error(testY[:, i], testPredict[:, i]))
        print(&#39;Test Percentage of relative error: {:.2f}&#39;.format(testRMSE))
        validateRMSE = math.sqrt(mean_squared_error(validateY[:, i], validatePredict[:, i]))
        print(&#39;Test Percentage of relative error: {:.2f}&#39;.format(validateRMSE))

        # shift train predictions for plotting
        trainPredictPlot = np.empty_like(dataset[:, i])
        trainPredictPlot[:] = np.nan
        trainPredictPlot[look_back:len(trainPredict) + look_back] = trainPredict[:, i]

        # shift test predictions for plotting
        testPredictPlot = np.empty_like(dataset[:, i])
        testPredictPlot[:] = np.nan
        testPredictPlot[
        len(trainPredict) + (look_back * 2) + 1:len(dataset) - len(validatePredict) - look_back - 2] = testPredict[:, i]

        # shift Validation predictions for plotting
        validatePredictPlot = np.empty_like(dataset[:, i])
        validatePredictPlot[:] = np.nan
        validatePredictPlot[-len(validatePredict):] = validatePredict[:, i]

        # plot baseline and predictions
        plt.figure()
        plt.plot(scaler.inverse_transform(dataset)[:, i], label=&#39;Original&#39;)
        plt.plot(trainPredictPlot, label=&#39;Train Forecast&#39;)
        plt.plot(testPredictPlot, label=&#39;Test Forecast&#39;)
        plt.plot(validatePredictPlot, label=&#39;Validation Forecast&#39;)
        plt.title(&#39;{}\nTrain Error : {:.3f}\nTest Error : {:.3f}&#39;.format(names[i], trainRMSE, testRMSE))

    plt.show()</code></pre>
                    </details>
                </dd>
            </dl>
        </section>
        <section>
        </section>
    </article>
    <nav id="sidebar">
        <h1>Index</h1>
        <div class="toc">
            <ul></ul>
        </div>
        <ul id="index">
            <li><h3>Super-module</h3>
                <ul>
                    <li><code><a href="index.html" title="Time_Series">Time_Series</a></code></li>
                </ul>
            </li>
            <li><h3><a href="#header-functions">Functions</a></h3>
                <ul class="">
                    <li><code><a href="#Time_Series.LSTM.create_lookback_dataset"
                                 title="Time_Series.LSTM.create_lookback_dataset">create_lookback_dataset</a></code>
                    </li>
                    <li><code><a href="#Time_Series.LSTM.main" title="Time_Series.LSTM.main">main</a></code></li>
                </ul>
            </li>
        </ul>
    </nav>
</main>
<footer id="footer">
    <p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>